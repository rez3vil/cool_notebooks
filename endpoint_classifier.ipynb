{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-19T12:01:38.372986Z",
     "start_time": "2022-07-19T12:01:38.366765Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# METADATA:\n",
    "__author__='Leela Sarath Kumar Konda'\n",
    "#########################################################################################################\n",
    "# README                                                                                                #\n",
    "#########################################################################################################\n",
    "# Before executing this code install the following                                                      #\n",
    "#                                                                                                       #\n",
    "# 1) anaconda        - Download the file from https://www.anaconda.com/ and                             #\n",
    "#                    - follow the given instructions                                                    #\n",
    "#                                                                                                       #\n",
    "# 2) rdkit           - conda install -c conda-forge rdkit                                               #\n",
    "#                                                                                                       #\n",
    "# 3) MolVS           - pip install MolVS                                                                #\n",
    "#                                                                                                       #\n",
    "# 4) java                                                                                               #\n",
    "#     linux          - sudo apt install default-jdk default-jre                                         #\n",
    "#     windows        - download the file from                                                           #\n",
    "#                    - https://www.oracle.com/in/java/technologies/javase-jdk15-downloads.html          #\n",
    "#                                                                                                       #\n",
    "# 5) easygui        - pip install easygui                                                               #\n",
    "#                                                                                                       #\n",
    "#########################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-19T12:01:39.055173Z",
     "start_time": "2022-07-19T12:01:38.628579Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import sys\n",
    "import warnings\n",
    "import os\n",
    "from os.path import join\n",
    "from os.path import splitext\n",
    "from os.path import dirname\n",
    "from os.path import basename\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.SaltRemover import SaltRemover\n",
    "from molvs import Standardizer\n",
    "from molvs.standardize import standardize_smiles\n",
    "\n",
    "import pandas\n",
    "import numpy\n",
    "\n",
    "import itertools\n",
    "from subprocess import call\n",
    "\n",
    "# import easygui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-19T12:01:39.062373Z",
     "start_time": "2022-07-19T12:01:39.057855Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# jar files\n",
    "_padel_jar_ = os.path.join(os.getcwd(), 'PaDEL-Descriptor', 'PaDEL-Descriptor.jar')\n",
    "_weka_jar_ = os.path.join(os.getcwd(), 'weka-3-9-3', 'weka.jar')\n",
    "\n",
    "#print(_padel_jar_)\n",
    "#print(_weka_jar_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-19T12:01:39.130212Z",
     "start_time": "2022-07-19T12:01:39.064072Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def std_smiles(smiles):\n",
    "    std = Standardizer()\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    std_mol = std.standardize(mol)\n",
    "    #std_mol = standardize_smiles(smiles) # from molvs, contains smiles to mol to smiles inbuilt rdkit function\n",
    "    return (std_mol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-19T12:01:39.204855Z",
     "start_time": "2022-07-19T12:01:39.190356Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def remove_salt(std_mol):\n",
    "    remover = SaltRemover()\n",
    "    #std_mol = Chem.MolFromSmiles(smiles) # uncomment this if we use standardize_smiles() method in std_smiles()\n",
    "    res = remover.StripMol(std_mol, dontRemoveEverything=True)\n",
    "    #res, deleted = remover.StripMolWithDeleted(mol)\n",
    "    #[Chem.MolToSmarts(m) for m in deleted]\n",
    "    if res.GetNumAtoms() <= 1:\n",
    "        return\n",
    "    else:\n",
    "        smiles = Chem.MolToSmiles(res)\n",
    "        return (smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-19T12:01:39.413642Z",
     "start_time": "2022-07-19T12:01:39.407688Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def standardframe(csv_file):\n",
    "    dataframe = pandas.read_csv(csv_file)\n",
    "    dataframe['Std_SMILES'] = [remove_salt(std_smiles(smiles)) for smiles in dataframe.SMILES]\n",
    "    std_dataframe = dataframe.dropna()\n",
    "    return (std_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-19T12:01:39.614929Z",
     "start_time": "2022-07-19T12:01:39.607556Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 1) (1266064593.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[6], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    No module named 'molvsdef smi_to_sdf(standard_records, csv_file):\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 1)\n"
     ]
    }
   ],
   "source": [
    "def smi_to_sdf(standard_records, csv_file):\n",
    "    sd_filename = '{}.sdf'.format(os.path.splitext(csv_file)[0])\n",
    "    sdwriter = Chem.SDWriter(sd_filename)\n",
    "    for mol, title in zip(standard_records.Std_SMILES, standard_records.ID):\n",
    "        mol = Chem.MolFromSmiles(mol)\n",
    "        hmol = Chem.AddHs(mol)\n",
    "        #AllChem.EmbedMolecule(hmol,AllChem.ETKDG())\n",
    "        #mol = Chem.RemoveHs(hmol)\n",
    "        hmol.SetProp(\"_Name\", title.strip())\n",
    "        sdwriter.write(hmol)\n",
    "    sdwriter.close()\n",
    "    return (sd_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-19T12:01:39.982939Z",
     "start_time": "2022-07-19T12:01:39.975094Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Data Preprocess\n",
    "# convert string to Nan\n",
    "def str_to_Nan(dataframe):\n",
    "    # to convert all non Numeric values to Nan\n",
    "    # and return the dataframe\n",
    "    # Iterate through columns of Pandas DataFrame\n",
    "    # Where string value exist replace with Nan\n",
    "\n",
    "    # Get list of DataFrame column names\n",
    "    cols = list(dataframe)\n",
    "    # Loop through columns\n",
    "    for column in cols:\n",
    "        # Transfer column to independent series\n",
    "        col_data = dataframe[column]\n",
    "        # Replace string data with Nan\n",
    "        string = pandas.to_numeric(col_data, errors='coerce')\n",
    "        dataframe[column] = string\n",
    "    return (dataframe)\n",
    "\n",
    "# Replace missing numerical data with median\n",
    "def fillNan(dataframe):\n",
    "    # to replace all Nan values with median of individual columns respectively\n",
    "    # and return the dataframe\n",
    "\n",
    "    data = str_to_Nan(dataframe)\n",
    "    #dataframe = data.fillna(data.median())#, inplace=True)\n",
    "    dataframe = data.fillna(data.median())\n",
    "    return (dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-19T12:01:40.518294Z",
     "start_time": "2022-07-19T12:01:40.508965Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def calc_features(std_dataframe, csv_file):\n",
    "\n",
    "    sdf_filename = smi_to_sdf(std_dataframe, csv_file)\n",
    "\n",
    "    # padel descriptor calculation:\n",
    "    # pass the smiles files\n",
    "    \n",
    "    _PARAMETERS = \"-threads -1 -2d -fingerprints -removesalt -standardizenitro -detectaromaticity -retainorder -descriptortypes \"\n",
    "    _fp_file = os.path.join(os.getcwd(), 'PaDEL-Descriptor', 'descriptors.xml')\n",
    "    pdl_output_file = '{}_pdl_desc.csv'.format(os.path.splitext(sdf_filename)[0])\n",
    "\n",
    "    command = 'java -jar -splash:disable {}'.format(_padel_jar_)\n",
    "    command += ' {}{}'.format(_PARAMETERS, _fp_file)\n",
    "    command += ' -dir {}'.format(sdf_filename)\n",
    "    command += ' -file {}'.format(pdl_output_file)\n",
    "\n",
    "    #running the command using subprocess.call\n",
    "    #call([command], shell=True)\n",
    "    os.system(command)\n",
    "\n",
    "    # impute with median values\n",
    "    # padel descriptor output file to pandas data frame\n",
    "    # and identify and replace the null values with\n",
    "    # median of respective column\n",
    "    dataframe = pandas.read_csv(pdl_output_file)\n",
    "    pdl_frame = fillNan(dataframe)\n",
    "    print(\"PaDEL Descriptor calculation was completed\")\n",
    "    return (pdl_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-19T12:01:41.036580Z",
     "start_time": "2022-07-19T12:01:41.029807Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def feature_selection(desc_set, feat_file):\n",
    "    features = pandas.read_csv(feat_file, header=None)\n",
    "    selected_features = desc_set[desc_set.columns.intersection(features[0].tolist())]\n",
    "    y_variable = features[0].tolist()[-1]\n",
    "    selected_features[y_variable] = '?'\n",
    "    return (selected_features, y_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-19T12:01:41.570943Z",
     "start_time": "2022-07-19T12:01:41.558370Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# WEKA\n",
    "def dataframetoarff(frame, y_variable, dtyp):  # pass the frame\n",
    "    arff = 'temp.arff'\n",
    "    colnames = frame.columns\n",
    "    datatypes = frame.dtypes\n",
    "    rows = frame.to_csv(header=None, index=None).split('\\n')\n",
    "\n",
    "    f = open(arff, \"w\")\n",
    "    f.write('@relation temp\\n\\n')\n",
    "\n",
    "    for name, dtype in zip(colnames, datatypes):\n",
    "        if name != y_variable:\n",
    "            f.write('@attribute {} numeric\\n'.format(name))\n",
    "        else:\n",
    "            f.write('@attribute {} {}\\n'.format(name, dtyp))\n",
    "\n",
    "    f.write('\\n@data\\n')\n",
    "\n",
    "    for row in rows:\n",
    "        f.write('{}\\n'.format(row))\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    return (arff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-19T12:01:42.001739Z",
     "start_time": "2022-07-19T12:01:41.989100Z"
    },
    "code_folding": [
     0,
     16
    ]
   },
   "outputs": [],
   "source": [
    "def get_ml_predictions(test_file, algo, model):\n",
    "    _arg_ = '-classifications weka.classifiers.evaluation.output.prediction.CSV'\n",
    "    csv = 'temp_result.csv'\n",
    "    command = 'java -splash:disable -Xmx8g -cp {} {}'.format(_weka_jar_, algo)\n",
    "    command += ' -l {}'.format(model)\n",
    "    command += ' -T {}'.format(test_file)\n",
    "    command += ' {}'.format(_arg_)\n",
    "    command += ' > {}'.format(csv)\n",
    "    #call([command], shell=True)\n",
    "    os.system(command)\n",
    "    predictions = pandas.read_csv(csv, delimiter=',', skiprows=4)\n",
    "    predictions.drop(predictions.columns[[0, 1, 3]], axis=1, inplace=True)\n",
    "    predictions.predicted.replace(regex={r'1:':'', '2:':''}, inplace=True)\n",
    "    #print(predictions)\n",
    "    return (predictions)\n",
    "\n",
    "def get_dl_predictions(test_frame, y_variable, model):\n",
    "    model = h2o.load_model(model)\n",
    "    #test_frame[y_variable].replace({'?': 1}, inplace=True)\n",
    "    test_frame = h2o.H2OFrame(test_frame[test_frame.columns[:-1]])\n",
    "    predictions = model.predict(test_frame)\n",
    "    predictions = predictions.as_data_frame(use_pandas=True, header=True)\n",
    "    #print(predictions)\n",
    "    return(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-19T12:01:42.224801Z",
     "start_time": "2022-07-19T12:01:42.212115Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_consensus(t1, t2, t3):\n",
    "    consensus = []\n",
    "    for pred6, pred5, pred4 in zip(t1, t2, t3):\n",
    "        if pred6 == \"Active\":\n",
    "            consensus.append(\"Strong\")\n",
    "        else:\n",
    "            if pred5 == \"Active\":\n",
    "                consensus.append(\"Moderate\")\n",
    "            else:\n",
    "                if pred4 == \"Active\":\n",
    "                    consensus.append(\"Weak\")\n",
    "                else:\n",
    "                    consensus.append(pred4)\n",
    "    return (consensus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T12:35:36.142141Z",
     "start_time": "2021-05-21T12:35:29.940865Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-19T12:02:07.052991Z",
     "start_time": "2022-07-19T12:02:02.615325Z"
    },
    "code_folding": [
     17,
     57
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'smi_to_sdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m std_dataframe \u001b[38;5;241m=\u001b[39m standardframe(input_file)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# get descriptor sets\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m desc_set \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstd_dataframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# empty dataframe to add the endpoint results\u001b[39;00m\n\u001b[1;32m     16\u001b[0m endpoint_pred \u001b[38;5;241m=\u001b[39m pandas\u001b[38;5;241m.\u001b[39mDataFrame()\n",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m, in \u001b[0;36mcalc_features\u001b[0;34m(std_dataframe, csv_file)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalc_features\u001b[39m(std_dataframe, csv_file):\n\u001b[0;32m----> 3\u001b[0m     sdf_filename \u001b[38;5;241m=\u001b[39m \u001b[43msmi_to_sdf\u001b[49m(std_dataframe, csv_file)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# padel descriptor calculation:\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# pass the smiles files\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     _PARAMETERS \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-threads -1 -2d -fingerprints -removesalt -standardizenitro -detectaromaticity -retainorder -descriptortypes \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'smi_to_sdf' is not defined"
     ]
    }
   ],
   "source": [
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "    # get the input file\n",
    "    # input_file = 'selected_vs-hits_list.csv'\n",
    "    # input_file = easygui.fileopenbox(title='Select the csv file',\n",
    "    #                                  default='*csv',\n",
    "    #                                  filetypes=['*.csv', 'CSV files'])\n",
    "    # print('Input file is {}\\n'.format(input_file))\n",
    "    input_file = 'example_file.csv'\n",
    "    # get standardized dataframe\n",
    "    std_dataframe = standardframe(input_file)\n",
    "    # get descriptor sets\n",
    "    desc_set = calc_features(std_dataframe, input_file)\n",
    "    # empty dataframe to add the endpoint results\n",
    "    endpoint_pred = pandas.DataFrame()\n",
    "    endpoint_pred['ID'] = std_dataframe.ID\n",
    "    endpoint_pred['SMILES'] = std_dataframe.Std_SMILES\n",
    "    # scan the current working directory\n",
    "    for folder in os.scandir(os.getcwd()):\n",
    "        if folder.is_dir():\n",
    "            #for BBB predictions\n",
    "            if folder.path == os.path.join(os.getcwd(), 'BBB'):\n",
    "                for feat_file in os.listdir(folder):\n",
    "                    if feat_file.endswith(\".txt\"):\n",
    "                        feat_file = os.path.join(folder, feat_file)\n",
    "                        # get selected features\n",
    "                        selected_features, y_variable = feature_selection(desc_set, feat_file)\n",
    "                        dtyp = '{BBB+,BBB-}'\n",
    "                        arff = dataframetoarff(selected_features, y_variable, dtyp)\n",
    "                        arff = os.path.join(os.getcwd(), arff)\n",
    "                        if feat_file == os.path.join(folder, 'bid_T2.txt'):\n",
    "                            for algo, model in zip(['MultilayerPerceptron', 'SMO'], ['MLP_BID_T2.model', 'SMO_BID_T2.model']):\n",
    "                                algo = 'weka.classifiers.functions.{}'.format(algo)\n",
    "                                model = os.path.join(os.getcwd(), 'BBB', model)\n",
    "                                predictions = get_ml_predictions(arff, algo, model)\n",
    "                                if 'SMO' in algo:\n",
    "                                    predictions.predicted.replace({'BBB+': 0.97, 'BBB-': -0.65}, inplace=True)\n",
    "                                    endpoint_pred['BBB-T2-SMO'] = predictions['predicted']\n",
    "                                else:\n",
    "                                    predictions.predicted.replace({'BBB+': 0.93, 'BBB-': -0.73}, inplace=True)\n",
    "                                    endpoint_pred['BBB-T2-MLP'] = predictions['predicted'] * predictions['prediction']\n",
    "                            \n",
    "                        elif feat_file == os.path.join(folder, 'bwd_T1.txt'):\n",
    "                            algo = 'weka.classifiers.functions.MultilayerPerceptron'\n",
    "                            model = os.path.join(os.getcwd(), 'BBB', 'MLP_BWD.model')\n",
    "                            predictions = get_ml_predictions(arff, algo, model)\n",
    "                            predictions.predicted.replace({'BBB+': 0.83, 'BBB-': -0.64}, inplace=True)\n",
    "                            endpoint_pred['BBB-T1-MLP'] = predictions['predicted'] * predictions['prediction']\n",
    "                            \n",
    "                        elif feat_file == os.path.join(folder, 'fwd_T1.txt'):\n",
    "                            algo = 'weka.classifiers.functions.SMO'\n",
    "                            model = os.path.join(os.getcwd(), 'BBB', 'SMO_FWD.model')\n",
    "                            predictions = get_ml_predictions(arff, algo, model)\n",
    "                            predictions.predicted.replace({'BBB+': 0.93, 'BBB-': -0.55}, inplace=True)\n",
    "                            endpoint_pred['BBB-T1-SMO'] = predictions['predicted']\n",
    "                endpoint_pred['BBB-T1-CONS'] = endpoint_pred[['BBB-T1-MLP', 'BBB-T1-SMO']].mean(axis=1).map(lambda x: 'BBB+' if x>0 else 'BBB-')\n",
    "                endpoint_pred.drop(['BBB-T1-MLP', 'BBB-T1-SMO'], axis=1, inplace=True)\n",
    "                endpoint_pred['BBB-T2-CONS'] = endpoint_pred[['BBB-T2-MLP', 'BBB-T2-SMO']].mean(axis=1).map(lambda x: 'BBB+' if x>0 else 'BBB-')\n",
    "                endpoint_pred.drop(['BBB-T2-MLP', 'BBB-T2-SMO'], axis=1, inplace=True)\n",
    "                                    \n",
    "            # for hERG predictions\n",
    "            elif folder.path == os.path.join(os.getcwd(), 'hERG'):\n",
    "                for feat_file in os.listdir(folder):\n",
    "                    if feat_file.endswith(\".txt\"):\n",
    "                        feat_file = os.path.join(folder, feat_file)\n",
    "                        # get selected features\n",
    "                        selected_features, y_variable = feature_selection(desc_set, feat_file)\n",
    "                        dtyp = '{Active,Inactive}'\n",
    "                        arff = dataframetoarff(selected_features, y_variable, dtyp)\n",
    "                        arff = os.path.join(os.getcwd(), arff)\n",
    "                        algo = 'weka.classifiers.trees.RandomForest'\n",
    "                        if feat_file == os.path.join(folder, '6_fwd.txt'):\n",
    "                            model = os.path.join(os.getcwd(), 'hERG', '6_fwd_RF.model')\n",
    "                            predictions = get_ml_predictions(arff, algo, model)\n",
    "                            endpoint_pred['hERG-01'] = predictions['predicted']\n",
    "                            #print('hERG-01\\n{}'.format(endpoint_pred['hERG-01']))\n",
    "                        elif feat_file == os.path.join(folder, '5_bwd.txt'):\n",
    "                            model = os.path.join(os.getcwd(), 'hERG', '5_bwd_RF.model')\n",
    "                            predictions = get_ml_predictions(arff, algo, model)\n",
    "                            endpoint_pred['hERG-10'] = predictions['predicted']\n",
    "                            #print('hERG-10\\n{}'.format(endpoint_pred['hERG-10']))\n",
    "                        elif feat_file == os.path.join(folder, '405_fwd.txt'):\n",
    "                            model = os.path.join(os.getcwd(), 'hERG', '4o5_fwd_RF.model')\n",
    "                            predictions = get_ml_predictions(arff, algo, model)\n",
    "                            predictions.predicted.replace({'Active': 1, 'Inactive': -1}, inplace=True)\n",
    "                            endpoint_pred['hERG-30-fwd'] = predictions['predicted'] * predictions['prediction']\n",
    "                            #print('hERG-30-fwd\\n{}'.format(endpoint_pred['hERG-30-fwd']))\n",
    "                        elif feat_file == os.path.join(folder, '405_bwd.txt'):\n",
    "                            model = os.path.join(os.getcwd(), 'hERG', '4o5_bwd_RF.model')\n",
    "                            predictions = get_ml_predictions(arff, algo, model)\n",
    "                            predictions.predicted.replace({'Active': 1, 'Inactive': -1}, inplace=True)\n",
    "                            endpoint_pred['hERG-30-bwd'] = predictions['predicted'] * predictions['prediction']\n",
    "                            #print('hERG-30-bwd\\n{}'.format(endpoint_pred['hERG-30-bwd']))\n",
    "                endpoint_pred['hERG-30'] = endpoint_pred[['hERG-30-fwd', 'hERG-30-bwd']].sum(axis=1).map(lambda x: 'Active' if x>0 else 'Inactive')\n",
    "                endpoint_pred.drop(['hERG-30-fwd','hERG-30-bwd'], axis=1, inplace=True)\n",
    "                consensus = get_consensus(endpoint_pred['hERG-01'], endpoint_pred['hERG-10'], endpoint_pred['hERG-30'])\n",
    "                endpoint_pred['hERG-CONS'] = consensus   \n",
    "            # for NaV predictions\n",
    "            # for CaV predictions\n",
    "            # for ABK predictions\n",
    "            # for FLT3 predictions\n",
    "\n",
    "    dir_name = dirname(input_file)\n",
    "    output_file = splitext(basename(input_file))[0]\n",
    "    endpoint_pred.to_csv('{}_results.csv'.format(join(dir_name, output_file), index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-19T11:48:05.623414Z",
     "start_time": "2022-07-19T11:48:05.623394Z"
    }
   },
   "outputs": [],
   "source": [
    "endpoint_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
